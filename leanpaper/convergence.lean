/-
Formalization of the convergence of Projected Gradient Descent for convex Lipschitz functions on a closed convex set.

We define the projection onto a closed convex set using the Hilbert projection theorem.
We define the Projected Gradient Descent (PGD) algorithm.
We prove the standard convergence rate O(1/sqrt(T)) for PGD on convex Lipschitz functions.

Main definitions:
- `projection`: The metric projection onto a closed convex set.
- `pgd_step`: One step of PGD.
- `pgd_sequence`: The sequence of iterates generated by PGD.

Main theorems:
- `projection_variational_inequality`: The variational inequality characterizing the projection.
- `pgd_step_descent`: A descent lemma for PGD.
- `pgd_convergence`: The main convergence theorem, stating that f(x_bar) - f(x*) <= epsilon, where x_bar is the average of the iterates.
-/

import Mathlib

set_option linter.mathlibStandardSet false

open scoped BigOperators
open scoped Real
open scoped Nat
open scoped Classical
open scoped Pointwise

set_option maxHeartbeats 0
set_option maxRecDepth 4000
set_option synthInstance.maxHeartbeats 20000
set_option synthInstance.maxSize 128

set_option relaxedAutoImplicit false
set_option autoImplicit false

noncomputable section

/-
Definition of projection onto a convex set using the Hilbert projection theorem.
-/
open InnerProductSpace Set

variable {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]

noncomputable def projection (K : Set E) (hK_nonempty : K.Nonempty) (hK_closed : IsClosed K) (hK_conv : Convex ℝ K) (x : E) : E :=
  Classical.choose (exists_norm_eq_iInf_of_complete_convex hK_nonempty hK_closed.isComplete hK_conv x)

/-
Specification and variational inequality for the projection onto a convex set.
-/
lemma projection_spec (K : Set E) (hK_nonempty : K.Nonempty) (hK_closed : IsClosed K) (hK_conv : Convex ℝ K) (x : E) :
    projection K hK_nonempty hK_closed hK_conv x ∈ K ∧
    ‖x - projection K hK_nonempty hK_closed hK_conv x‖ = ⨅ w : K, ‖x - w‖ :=
  Classical.choose_spec (exists_norm_eq_iInf_of_complete_convex hK_nonempty hK_closed.isComplete hK_conv x)

lemma projection_variational_inequality (K : Set E) (hK_nonempty : K.Nonempty) (hK_closed : IsClosed K) (hK_conv : Convex ℝ K) (x : E) (y : E) (hy : y ∈ K) :
    inner ℝ (x - projection K hK_nonempty hK_closed hK_conv x) (y - projection K hK_nonempty hK_closed hK_conv x) ≤ (0 : ℝ) := by
  -- By definition of projection, we know that for any $y \in K$, we have $\|x - \text{projection}(K, x)\|^2 \leq \|x - y\|^2$.
  have h_dist : ∀ y ∈ K, ‖x - (projection K hK_nonempty hK_closed hK_conv x)‖^2 ≤ ‖x - y‖^2 := by
    have h_min_dist : ∀ y ∈ K, ‖x - (projection K hK_nonempty hK_closed hK_conv x)‖ ≤ ‖x - y‖ := by
      unfold projection; aesop;
      have := Classical.choose_spec ( ( exists_norm_eq_iInf_of_complete_convex hK_nonempty hK_closed.isComplete hK_conv x ) ) ; aesop;
      exact ciInf_le_of_le ⟨ 0, Set.forall_mem_range.2 fun _ => norm_nonneg _ ⟩ ⟨ y_1, a ⟩ le_rfl;
    exact fun y hy => pow_le_pow_left₀ ( norm_nonneg _ ) ( h_min_dist y hy ) 2;
  -- By definition of projection, we know that for any $y \in K$, we have $\|x - \text{projection}(K, x)\|^2 \leq \|x - y\|^2$. Let's choose $y = (1 - t)\text{projection}(K, x) + t y$ for $t \in [0, 1]$.
  have h_dist_t : ∀ t ∈ Set.Icc (0 : ℝ) 1, ‖x - projection K hK_nonempty hK_closed hK_conv x‖^2 ≤ ‖x - ((1 - t) • projection K hK_nonempty hK_closed hK_conv x + t • y)‖^2 := by
    aesop;
    apply h_dist;
    exact hK_conv ( Classical.choose_spec ( exists_norm_eq_iInf_of_complete_convex hK_nonempty hK_closed.isComplete hK_conv x ) |>.1 ) hy ( by linarith ) ( by linarith ) ( by linarith );
  -- Expanding the right-hand side of the inequality $\|x - ((1 - t)\text{projection}(K, x) + t y)\|^2$, we get $\|x - \text{projection}(K, x) - t(y - \text{projection}(K, x))\|^2$.
  have h_expand : ∀ t ∈ Set.Icc (0 : ℝ) 1, ‖x - ((1 - t) • projection K hK_nonempty hK_closed hK_conv x + t • y)‖^2 = ‖x - projection K hK_nonempty hK_closed hK_conv x‖^2 - 2 * t * ⟪x - projection K hK_nonempty hK_closed hK_conv x, y - projection K hK_nonempty hK_closed hK_conv x⟫_ℝ + t^2 * ‖y - projection K hK_nonempty hK_closed hK_conv x‖^2 := by
    simp +decide [ norm_add_sq_real, norm_sub_sq_real, inner_add_left, inner_sub_left, inner_add_right, inner_sub_right, inner_smul_left, inner_smul_right ];
    intro t ht ht'; rw [ norm_smul, norm_smul ] ; norm_num [ real_inner_comm ] ; ring;
    rw [ abs_of_nonneg ( sub_nonneg.2 ht' ), abs_of_nonneg ht ] ; rw [ real_inner_self_eq_norm_sq ] ; ring;
  -- Dividing both sides of the inequality by $t$ (since $t > 0$), we get $-2 * ⟪x - projection K hK_nonempty hK_closed hK_conv x, y - projection K hK_nonempty hK_closed hK_conv x⟫_ℝ + t * ‖y - projection K hK_nonempty hK_closed hK_conv x‖^2 ≥ 0$.
  have h_div : ∀ t ∈ Set.Ioo (0 : ℝ) 1, -2 * ⟪x - projection K hK_nonempty hK_closed hK_conv x, y - projection K hK_nonempty hK_closed hK_conv x⟫_ℝ + t * ‖y - projection K hK_nonempty hK_closed hK_conv x‖^2 ≥ 0 := by
    intro t ht; nlinarith [ h_expand t ⟨ ht.1.le, ht.2.le ⟩, h_dist_t t ⟨ ht.1.le, ht.2.le ⟩, ht.1, ht.2 ] ;
  -- Taking the limit as $t$ approaches $0$ from the right, we get $-2 * ⟪x - projection K hK_nonempty hK_closed hK_conv x, y - projection K hK_nonempty hK_closed hK_conv x⟫_ℝ \geq 0$.
  have h_lim : Filter.Tendsto (fun t : ℝ => -2 * ⟪x - projection K hK_nonempty hK_closed hK_conv x, y - projection K hK_nonempty hK_closed hK_conv x⟫_ℝ + t * ‖y - projection K hK_nonempty hK_closed hK_conv x‖^2) (nhdsWithin 0 (Set.Ioi 0)) (nhds (-2 * ⟪x - projection K hK_nonempty hK_closed hK_conv x, y - projection K hK_nonempty hK_closed hK_conv x⟫_ℝ)) := by
    exact tendsto_nhdsWithin_of_tendsto_nhds ( Continuous.tendsto' ( by continuity ) _ _ ( by simp +decide ) );
  exact le_of_not_gt fun h => absurd ( le_of_tendsto_of_tendsto tendsto_const_nhds h_lim <| Filter.eventually_of_mem ( Ioo_mem_nhdsGT_of_mem ⟨ by norm_num, by norm_num ⟩ ) h_div ) ( by linarith )

/-
Definition of Projected Gradient Descent step and sequence.
-/
noncomputable def pgd_step (K : Set E) (hK_nonempty : K.Nonempty) (hK_closed : IsClosed K) (hK_conv : Convex ℝ K)
    (f : E → ℝ) (η : ℝ) (x : E) : E :=
  projection K hK_nonempty hK_closed hK_conv (x - η • gradient f x)

noncomputable def pgd_sequence (K : Set E) (hK_nonempty : K.Nonempty) (hK_closed : IsClosed K) (hK_conv : Convex ℝ K)
    (f : E → ℝ) (η : ℝ) (x0 : E) : ℕ → E
  | 0 => x0
  | n + 1 => pgd_step K hK_nonempty hK_closed hK_conv f η (pgd_sequence K hK_nonempty hK_closed hK_conv f η x0 n)

/-
Bound on the norm of the gradient for a Lipschitz function.
-/
lemma norm_gradient_le_of_lipschitz {f : E → ℝ} (hf_diff : Differentiable ℝ f) (L : NNReal) (hf_lip : LipschitzWith L f) (x : E) :
    ‖gradient f x‖ ≤ L := by
  -- Since the gradient is the Riesz representer of the Fréchet derivative, and the Riesz representation is an isometry, we have that the norm of the gradient is equal to the norm of the derivative.
  have h_riesz : ‖gradient f x‖ = ‖fderiv ℝ f x‖ := by
    simp +decide [ gradient ];
  rw [ h_riesz ];
  exact?

/-
Descent lemma for a single step of Projected Gradient Descent.
-/
lemma pgd_step_descent (K : Set E) (hK_nonempty : K.Nonempty) (hK_closed : IsClosed K) (hK_conv : Convex ℝ K)
    (f : E → ℝ) (η : ℝ) (x : E) (y : E) (hy : y ∈ K) :
    ‖pgd_step K hK_nonempty hK_closed hK_conv f η x - y‖^2 ≤ ‖x - y‖^2 - 2 * η * inner ℝ (gradient f x) (x - y) + η^2 * ‖gradient f x‖^2 := by
  -- Let $x_{new} = \text{pgd\_step}(x)$. By definition, $x_{new} = \text{proj}_K(x - \eta \nabla f(x))$.
  set x_new := pgd_step K hK_nonempty hK_closed hK_conv f η x;
  -- By the non-expansiveness of the projection onto a convex set, we have:
  have h_nonexp : ‖x_new - y‖ ^ 2 ≤ ‖(x - η • gradient f x) - y‖ ^ 2 := by
    have h_nonexp : ∀ z ∈ K, ‖x_new - z‖ ≤ ‖(x - η • gradient f x) - z‖ := by
      intro z hz
      have h_proj : x_new = projection K hK_nonempty hK_closed hK_conv (x - η • gradient f x) := by
        rfl
      rw [h_proj]
      exact (by
      have := projection_variational_inequality K hK_nonempty hK_closed hK_conv ( x - η • gradient f x ) z hz;
      have := norm_add_sq_real ( x - η • gradient f x - projection K hK_nonempty hK_closed hK_conv ( x - η • gradient f x ) ) ( projection K hK_nonempty hK_closed hK_conv ( x - η • gradient f x ) - z );
      simp_all +decide [ inner_sub_left, inner_sub_right ];
      exact le_of_sub_nonneg ( by nlinarith [ norm_nonneg ( x - η • gradient f x - z ), norm_nonneg ( x - η • gradient f x - projection K hK_nonempty hK_closed hK_conv ( x - η • gradient f x ) ), norm_nonneg ( projection K hK_nonempty hK_closed hK_conv ( x - η • gradient f x ) - z ) ] ));
    exact pow_le_pow_left₀ ( norm_nonneg _ ) ( h_nonexp y hy ) 2;
  rw [ @norm_sub_sq ℝ ] at *;
  rw [ @norm_sub_sq ℝ, @norm_sub_sq ℝ ] at *;
  simp_all +decide [ inner_sub_left, inner_sub_right, inner_smul_left, inner_smul_right, norm_smul ];
  norm_num [ mul_pow, mul_assoc, mul_comm, mul_left_comm, real_inner_comm ] at * ; linarith

/-
Bound on the sum of function value differences for Projected Gradient Descent.
-/
lemma pgd_sum_bound
    (K : Set E) (hK_nonempty : K.Nonempty) (hK_closed : IsClosed K) (hK_conv : Convex ℝ K)
    (f : E → ℝ) (hf_conv : ConvexOn ℝ K f) (hf_diff : Differentiable ℝ f) (L : NNReal) (hf_lip : LipschitzWith L f)
    (x_star : E) (hx_star : x_star ∈ K)
    (x0 : E) (hx0 : x0 ∈ K)
    (T : ℕ) (η : ℝ) (hη : 0 < η) :
    ∑ t ∈ Finset.range (T + 1), (f (pgd_sequence K hK_nonempty hK_closed hK_conv f η x0 t) - f x_star) ≤
    ‖x0 - x_star‖^2 / (2 * η) + (T + 1 : ℝ) * η * L^2 / 2 := by
  -- Apply the descent lemma to each term in the sum.
  have h_step (t : ℕ) : f (pgd_sequence K hK_nonempty hK_closed hK_conv f η x0 t) - f x_star ≤ (‖(pgd_sequence K hK_nonempty hK_closed hK_conv f η x0 t) - x_star‖^2 - ‖(pgd_sequence K hK_nonempty hK_closed hK_conv f η x0 (t + 1)) - x_star‖^2) / (2 * η) + η * (L : ℝ)^2 / 2 := by
    -- Applying the descent lemma and the bound on the norm of the gradient, we get:
    have h_step : 2 * η * (f (pgd_sequence K hK_nonempty hK_closed hK_conv f η x0 t) - f x_star) ≤ ‖(pgd_sequence K hK_nonempty hK_closed hK_conv f η x0 t) - x_star‖^2 - ‖(pgd_sequence K hK_nonempty hK_closed hK_conv f η x0 (t + 1)) - x_star‖^2 + η^2 * (L : ℝ)^2 := by
      have h_step : 2 * η * (f (pgd_sequence K hK_nonempty hK_closed hK_conv f η x0 t) - f x_star) ≤ ‖(pgd_sequence K hK_nonempty hK_closed hK_conv f η x0 t) - x_star‖^2 - ‖(pgd_sequence K hK_nonempty hK_closed hK_conv f η x0 (t + 1)) - x_star‖^2 + η^2 * ‖(gradient f (pgd_sequence K hK_nonempty hK_closed hK_conv f η x0 t))‖^2 := by
        -- Apply the descent lemma to each term in the sum, using the fact that $f$ is convex and differentiable.
        have h_apply_descent : ∀ x y : E, x ∈ K → y ∈ K → 2 * η * (f x - f y) ≤ ‖x - y‖^2 - ‖pgd_step K hK_nonempty hK_closed hK_conv f η x - y‖^2 + η^2 * ‖gradient f x‖^2 := by
          -- By definition of the projection, we have $\langle \nabla f(x), x - y \rangle \geq f(x) - f(y)$.
          have h_inner : ∀ x y : E, x ∈ K → y ∈ K → inner ℝ (gradient f x) (x - y) ≥ f x - f y := by
            intro x y hx hy
            have h_inner : ∀ t ∈ Set.Ioo (0 : ℝ) 1, (f (x + t • (y - x)) - f x) / t ≤ f y - f x := by
              intro t ht
              have h_inner : f (x + t • (y - x)) ≤ (1 - t) * f x + t * f y := by
                convert hf_conv.2 hx hy ( by linarith [ ht.1, ht.2 ] : 0 ≤ 1 - t ) ( by linarith [ ht.1, ht.2 ] : 0 ≤ t ) ( by linarith [ ht.1, ht.2 ] ) using 1 ; simp +decide [ add_comm, smul_sub ];
                simp +decide [ sub_smul, add_comm, add_left_comm, add_assoc ];
                exact congr_arg f ( by abel1 );
              rw [ div_le_iff₀ ] <;> linarith [ ht.1, ht.2 ];
            -- Taking the limit as $t \to 0$ in the inequality $\frac{f(x + t(y - x)) - f(x)}{t} \leq f(y) - f(x)$, we get $\langle \nabla f(x), y - x \rangle \leq f(y) - f(x)$.
            have h_limit : Filter.Tendsto (fun t : ℝ => (f (x + t • (y - x)) - f x) / t) (nhdsWithin 0 (Set.Ioi 0)) (nhds (inner ℝ (gradient f x) (y - x))) := by
              have h_limit : HasDerivAt (fun t : ℝ => f (x + t • (y - x))) (inner ℝ (gradient f x) (y - x)) 0 := by
                convert HasFDerivAt.hasDerivAt ( HasFDerivAt.comp 0 ( hf_diff.differentiableAt.hasFDerivAt ) ( HasFDerivAt.add ( hasFDerivAt_const _ _ ) ( HasFDerivAt.smul ( hasFDerivAt_id 0 ) ( hasFDerivAt_const _ _ ) ) ) ) using 1 ; simp +decide [ gradient ];
              simpa [ div_eq_inv_mul ] using h_limit.tendsto_slope_zero_right;
            norm_num +zetaDelta at *;
            have := le_of_tendsto h_limit ( Filter.eventually_of_mem ( Ioo_mem_nhdsGT_of_mem ⟨ le_rfl, zero_lt_one ⟩ ) fun t ht => h_inner t ht.1 ht.2 );
            rw [ show x - y = - ( y - x ) by abel1, inner_neg_right ] ; linarith;
          intro x y hx hy
          have h_apply_descent : ‖pgd_step K hK_nonempty hK_closed hK_conv f η x - y‖^2 ≤ ‖x - y‖^2 - 2 * η * inner ℝ (gradient f x) (x - y) + η^2 * ‖gradient f x‖^2 := by
            convert pgd_step_descent K hK_nonempty hK_closed hK_conv f η x y hy using 1;
          nlinarith [ h_inner x y hx hy ];
        convert h_apply_descent _ _ _ hx_star;
        induction' t with t ih;
        · exact hx0;
        · exact Classical.choose_spec ( exists_norm_eq_iInf_of_complete_convex hK_nonempty hK_closed.isComplete hK_conv _ ) |>.1;
      refine le_trans h_step ?_;
      gcongr;
      exact?;
    rw [ div_add_div, le_div_iff₀ ] <;> nlinarith;
  refine' le_trans ( Finset.sum_le_sum fun i hi => h_step i ) _;
  norm_num [ Finset.sum_add_distrib, ← Finset.sum_div ];
  rw [ Finset.sum_range_succ' ];
  norm_num [ Finset.sum_range_succ ];
  exact add_le_add ( div_le_div_of_nonneg_right ( sub_le_self _ ( sq_nonneg _ ) ) ( by positivity ) ) ( by ring_nf; norm_num )

/-
Convergence theorem for Projected Gradient Descent on a convex set.
-/
theorem pgd_convergence
    (K : Set E) (hK_nonempty : K.Nonempty) (hK_closed : IsClosed K) (hK_conv : Convex ℝ K)
    (f : E → ℝ) (hf_conv : ConvexOn ℝ K f) (hf_diff : Differentiable ℝ f) (L : NNReal) (hf_lip : LipschitzWith L f)
    (x_star : E) (hx_star : x_star ∈ K) (h_min : IsMinOn f K x_star)
    (x0 : E) (hx0 : x0 ∈ K)
    (ε : ℝ) (hε : 0 < ε)
    (T : ℕ) (hT : (T : ℝ) ≥ (L : ℝ)^2 * ‖x0 - x_star‖^2 / ε^2)
    (η : ℝ) (hη : η = ‖x0 - x_star‖ / ((L : ℝ) * Real.sqrt T)) :
    f ((T + 1 : ℝ)⁻¹ • ∑ t ∈ Finset.range (T + 1), pgd_sequence K hK_nonempty hK_closed hK_conv f η x0 t) - f x_star ≤ ε := by
  by_cases hL : L = 0 <;> by_cases hT : T = 0 <;> simp_all +decide;
  · have := hf_lip.dist_le_mul x0 x_star; simp_all +decide [ IsMinOn ] ;
    linarith!;
  · simp_all +decide [ LipschitzWith ];
    rw [ hf_lip _ x_star ] ; linarith;
  · rw [ div_le_iff₀ ] at * <;> aesop;
    rw [ le_iff_lt_or_eq ] at hT_1 ; aesop;
    · exact False.elim <| h.not_le <| by positivity;
    · rw [ sub_eq_zero.mp h_1 ] ; exact le_add_of_nonneg_left hε.le;
  · -- By Jensen's inequality:
    have h_jensen : f (((T + 1 : ℝ)⁻¹ • ∑ t ∈ Finset.range (T + 1), pgd_sequence K hK_nonempty hK_closed hK_conv f (‖x0 - x_star‖ / ((L : ℝ) * Real.sqrt T)) x0 t)) ≤ (T + 1 : ℝ)⁻¹ * ∑ t ∈ Finset.range (T + 1), f (pgd_sequence K hK_nonempty hK_closed hK_conv f (‖x0 - x_star‖ / ((L : ℝ) * Real.sqrt T)) x0 t) := by
      have h_jensen : ∀ (s : Finset ℕ) (w : ℕ → ℝ), (∀ t ∈ s, 0 ≤ w t) → (∑ t ∈ s, w t = 1) → (∀ t ∈ s, pgd_sequence K hK_nonempty hK_closed hK_conv f (‖x0 - x_star‖ / ((L : ℝ) * Real.sqrt T)) x0 t ∈ K) → f (∑ t ∈ s, w t • pgd_sequence K hK_nonempty hK_closed hK_conv f (‖x0 - x_star‖ / ((L : ℝ) * Real.sqrt T)) x0 t) ≤ ∑ t ∈ s, w t * f (pgd_sequence K hK_nonempty hK_closed hK_conv f (‖x0 - x_star‖ / ((L : ℝ) * Real.sqrt T)) x0 t) := by
        intro s w hw hw' hw''; exact hf_conv.map_sum_le ( by aesop ) ( by aesop ) ( by aesop ) ;
      convert h_jensen ( Finset.range ( T + 1 ) ) ( fun _ => ( T + 1 : ℝ ) ⁻¹ ) ( fun _ _ => by positivity ) ( by simp +decide [ Finset.sum_const, Finset.card_range, ne_of_gt ( Nat.cast_add_one_pos T ) ] ) ( fun t ht => by
        induction' t with t ih;
        · exact hx0;
        · exact Classical.choose_spec ( exists_norm_eq_iInf_of_complete_convex hK_nonempty hK_closed.isComplete hK_conv _ ) |>.1 ) using 1 <;> norm_num [ Finset.mul_sum _ _ _, Finset.sum_mul _ _ _, smul_smul ];
      rw [ Finset.smul_sum ];
    -- By Lemma `pgd_sum_bound`, we have:
    have h_sum_bound : ∑ t ∈ Finset.range (T + 1), (f (pgd_sequence K hK_nonempty hK_closed hK_conv f (‖x0 - x_star‖ / ((L : ℝ) * Real.sqrt T)) x0 t) - f x_star) ≤ ‖x0 - x_star‖^2 / (2 * (‖x0 - x_star‖ / ((L : ℝ) * Real.sqrt T))) + (T + 1 : ℝ) * (‖x0 - x_star‖ / ((L : ℝ) * Real.sqrt T)) * L^2 / 2 := by
      by_cases h : ‖x0 - x_star‖ = 0;
      · simp_all +decide [ sub_eq_zero ];
        -- Since $x_0 = x^*$, we have $pgd_sequence K hK_nonempty hK_closed hK_conv f 0 x_star x = x^*$ for all $x$.
        have h_pgd_seq : ∀ x : ℕ, pgd_sequence K hK_nonempty hK_closed hK_conv f 0 x_star x = x_star := by
          intro x; induction x <;> simp_all +decide [ pgd_sequence ] ;
          unfold pgd_step;
          simp +decide [ projection_spec ];
          have := projection_spec K hK_nonempty hK_closed hK_conv x_star;
          rw [ eq_comm, ← sub_eq_zero ];
          refine' norm_eq_zero.mp ( this.2.trans ( le_antisymm _ _ ) );
          · exact le_trans ( ciInf_le ⟨ 0, Set.forall_mem_range.2 fun _ => norm_nonneg _ ⟩ ⟨ x_star, hx_star ⟩ ) ( by simp +decide );
          · exact Real.iInf_nonneg fun _ => norm_nonneg _;
        aesop;
      · apply pgd_sum_bound;
        all_goals first | positivity | assumption;
    -- Substitute the bound from `h_sum_bound` into the inequality from `h_jensen`.
    have h_subst : f (((T + 1 : ℝ)⁻¹ • ∑ t ∈ Finset.range (T + 1), pgd_sequence K hK_nonempty hK_closed hK_conv f (‖x0 - x_star‖ / ((L : ℝ) * Real.sqrt T)) x0 t)) ≤ f x_star + (‖x0 - x_star‖ * L * Real.sqrt T) / (2 * (T + 1)) + (‖x0 - x_star‖ * L) / (2 * Real.sqrt T) := by
      refine le_trans h_jensen ?_;
      rw [ inv_mul_le_iff₀ ( by positivity ) ];
      by_cases h : ‖x0 - x_star‖ = 0 <;> simp_all +decide [ division_def, mul_assoc, mul_comm, mul_left_comm ];
      convert h_sum_bound using 1 ; ring;
      -- Combine like terms and simplify the expression.
      field_simp
      ring;
    -- Since $\frac{\sqrt{T}}{T+1} < \frac{1}{\sqrt{T}}$, the sum is bounded by $\frac{\|x_0 - x^*\| L}{\sqrt{T}}$.
    have h_bound : (‖x0 - x_star‖ * L * Real.sqrt T) / (2 * (T + 1)) + (‖x0 - x_star‖ * L) / (2 * Real.sqrt T) ≤ ‖x0 - x_star‖ * L / Real.sqrt T := by
      field_simp;
      rw [ Real.sq_sqrt ( Nat.cast_nonneg _ ) ] ; nlinarith only [ show ( 1 : ℝ ) ≤ T by exact Nat.one_le_cast.mpr ( Nat.pos_of_ne_zero hT ), norm_nonneg ( x0 - x_star ) ];
    -- Using the given condition on $T$, we have $\frac{\|x_0 - x^*\| L}{\sqrt{T}} \leq \epsilon$.
    have h_eps : ‖x0 - x_star‖ * L / Real.sqrt T ≤ ε := by
      rw [ div_le_iff₀ ( by positivity ) ] at *;
      nlinarith [ show 0 ≤ ε * Real.sqrt T by positivity, Real.mul_self_sqrt ( Nat.cast_nonneg T ) ];
    linarith
