/-
This file was generated by Aristotle.

Lean version: leanprover/lean4:v4.24.0
Mathlib version: f897ebcf72cd16f89ab4577d0c826cd14afaafc7
This project request had uuid: 3fb3bfcc-3c7f-4f66-9857-680d45d9a599

Aristotle's budget for this request has been reached.
If there is partial progress, it will appear in this file.
If you would like to continue working off of this partial progress,
please submit the same prompt, and add this .lean file in "Optional: Attach a Lean file as context" field.

-/

import Mathlib

set_option linter.mathlibStandardSet false

open scoped BigOperators
open scoped Real
open scoped Nat
open scoped Classical
open scoped Pointwise

set_option maxHeartbeats 0
set_option maxRecDepth 4000
set_option synthInstance.maxHeartbeats 20000
set_option synthInstance.maxSize 128

set_option relaxedAutoImplicit false
set_option autoImplicit false

noncomputable section

/-
Definition of projection onto a closed convex set in a Hilbert space.
-/
noncomputable def convexProj {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]
  (K : Set E) (hK_ne : K.Nonempty) (hK_cl : IsClosed K) (hK_conv : Convex ℝ K) (x : E) : E :=
  Classical.choose (exists_norm_eq_iInf_of_complete_convex hK_ne hK_cl.isComplete hK_conv x)

lemma convexProj_mem {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]
  (K : Set E) (hK_ne : K.Nonempty) (hK_cl : IsClosed K) (hK_conv : Convex ℝ K) (x : E) :
  convexProj K hK_ne hK_cl hK_conv x ∈ K :=
  (Classical.choose_spec (exists_norm_eq_iInf_of_complete_convex hK_ne hK_cl.isComplete hK_conv x)).1

/-
Projected Gradient Descent step and sequence definitions.
-/
noncomputable def pgd_step {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]
  (K : Set E) (hK_ne : K.Nonempty) (hK_cl : IsClosed K) (hK_conv : Convex ℝ K)
  (f : E → ℝ) (η : ℝ) (x : E) : E :=
  convexProj K hK_ne hK_cl hK_conv (x - η • gradient f x)

noncomputable def pgd_sequence {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]
  (K : Set E) (hK_ne : K.Nonempty) (hK_cl : IsClosed K) (hK_conv : Convex ℝ K)
  (f : E → ℝ) (η : ℝ) (x₀ : E) : ℕ → E
| 0 => x₀
| (n + 1) => pgd_step K hK_ne hK_cl hK_conv f η (pgd_sequence K hK_ne hK_cl hK_conv f η x₀ n)

/-
Checking inner and Real.toNNReal.
-/
#check inner
#check Real.toNNReal

/-
Variational inequality for the projection onto a closed convex set.
-/
lemma convexProj_variational_inequality {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]
  (K : Set E) (hK_ne : K.Nonempty) (hK_cl : IsClosed K) (hK_conv : Convex ℝ K)
  (x : E) (y : E) (hy : y ∈ K) :
  inner ℝ (x - convexProj K hK_ne hK_cl hK_conv x) (y - convexProj K hK_ne hK_cl hK_conv x) ≤ 0 := by
    -- By definition of the projection, we know that $\|x - y\|^2 \geq \|x - \pi_K(x)\|^2$ for all $y \in K$.
    have h_proj_prop : ∀ y ∈ K, ‖x - y‖^2 ≥ ‖x - convexProj K hK_ne hK_cl hK_conv x‖^2 := by
      -- By definition of convexProj, we know that for any y ∈ K, ‖x - convexProj K hK_ne hK_cl hK_conv x‖ ≤ ‖x - y‖.
      have h_proj : ∀ y ∈ K, ‖x - convexProj K hK_ne hK_cl hK_conv x‖ ≤ ‖x - y‖ := by
        have := Classical.choose_spec ( exists_norm_eq_iInf_of_complete_convex hK_ne hK_cl.isComplete hK_conv x );
        exact fun y hy => this.2.symm ▸ le_trans ( ciInf_le ⟨ 0, Set.forall_mem_range.2 fun _ => norm_nonneg _ ⟩ ⟨ y, hy ⟩ ) ( by simp +decide );
      exact fun y hy => pow_le_pow_left₀ ( norm_nonneg _ ) ( h_proj y hy ) 2;
    have h_proj_prop' : ∀ t ∈ Set.Icc (0 : ℝ) 1, ‖x - (t • y + (1 - t) • convexProj K hK_ne hK_cl hK_conv x)‖^2 ≥ ‖x - convexProj K hK_ne hK_cl hK_conv x‖^2 := by
      exact fun t ht => h_proj_prop _ ( hK_conv hy ( convexProj_mem K hK_ne hK_cl hK_conv x ) ht.1 ( sub_nonneg.2 ht.2 ) ( by simp +decide ) );
    -- Expanding the left-hand side of the inequality, we get:
    have h_expand : ∀ t ∈ Set.Icc (0 : ℝ) 1, ‖x - convexProj K hK_ne hK_cl hK_conv x‖^2 + 2 * t * inner ℝ (x - convexProj K hK_ne hK_cl hK_conv x) (convexProj K hK_ne hK_cl hK_conv x - y) + t^2 * ‖convexProj K hK_ne hK_cl hK_conv x - y‖^2 ≥ ‖x - convexProj K hK_ne hK_cl hK_conv x‖^2 := by
      intro t ht
      specialize h_proj_prop' t ht
      have h_expand : ‖x - (t • y + (1 - t) • convexProj K hK_ne hK_cl hK_conv x)‖^2 = ‖x - convexProj K hK_ne hK_cl hK_conv x‖^2 + 2 * t * inner ℝ (x - convexProj K hK_ne hK_cl hK_conv x) (convexProj K hK_ne hK_cl hK_conv x - y) + t^2 * ‖convexProj K hK_ne hK_cl hK_conv x - y‖^2 := by
        simp +decide only [norm_sub_sq_real, inner_sub_right, inner_sub_left] ; ring;
        simp +decide [ norm_add_sq_real, norm_smul, inner_add_left, inner_add_right, inner_smul_left, inner_smul_right ] ; ring;
        rw [ abs_of_nonneg ht.1, abs_of_nonneg ( sub_nonneg.2 ht.2 ) ] ; simp +decide [ real_inner_comm, real_inner_self_eq_norm_sq ] ; ring;
      linarith;
    -- By taking the limit as $t \to 0$, we get:
    have h_limit : Filter.Tendsto (fun t : ℝ => (2 * t * inner ℝ (x - convexProj K hK_ne hK_cl hK_conv x) (convexProj K hK_ne hK_cl hK_conv x - y) + t^2 * ‖convexProj K hK_ne hK_cl hK_conv x - y‖^2) / t) (nhdsWithin 0 (Set.Ioi 0)) (nhds (2 * inner ℝ (x - convexProj K hK_ne hK_cl hK_conv x) (convexProj K hK_ne hK_cl hK_conv x - y))) := by
      have h_limit : Filter.Tendsto (fun t : ℝ => 2 * inner ℝ (x - convexProj K hK_ne hK_cl hK_conv x) (convexProj K hK_ne hK_cl hK_conv x - y) + t * ‖convexProj K hK_ne hK_cl hK_conv x - y‖^2) (nhdsWithin 0 (Set.Ioi 0)) (nhds (2 * inner ℝ (x - convexProj K hK_ne hK_cl hK_conv x) (convexProj K hK_ne hK_cl hK_conv x - y))) := by
        exact tendsto_nhdsWithin_of_tendsto_nhds ( Continuous.tendsto' ( by continuity ) _ _ ( by simp +decide ) );
      refine' h_limit.congr' ( by filter_upwards [ self_mem_nhdsWithin ] with t ht using by rw [ eq_div_iff ht.out.ne' ] ; ring );
    -- Since the limit is non-negative, we have:
    have h_nonneg : 2 * inner ℝ (x - convexProj K hK_ne hK_cl hK_conv x) (convexProj K hK_ne hK_cl hK_conv x - y) ≥ 0 := by
      exact le_of_tendsto_of_tendsto tendsto_const_nhds h_limit ( Filter.eventually_of_mem ( Ioo_mem_nhdsGT_of_mem ⟨ le_rfl, zero_lt_one ⟩ ) fun t ht => by rw [ le_div_iff₀ ht.1 ] ; linarith [ h_expand t ⟨ ht.1.le, ht.2.le ⟩ ] );
    simp_all +decide [ inner_sub_left, inner_sub_right ]

/-
Non-expansiveness of the projection onto a closed convex set.
-/
lemma norm_convexProj_sub_le {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]
  (K : Set E) (hK_ne : K.Nonempty) (hK_cl : IsClosed K) (hK_conv : Convex ℝ K)
  (z : E) (hz : z ∈ K) (y : E) :
  ‖convexProj K hK_ne hK_cl hK_conv y - z‖ ≤ ‖y - z‖ := by
    have := convexProj_variational_inequality K hK_ne hK_cl hK_conv y z hz;
    -- By expanding the squared norm of the difference, we can use the variational inequality to conclude the proof.
    have h_expand : ‖y - z‖^2 = ‖y - convexProj K hK_ne hK_cl hK_conv y‖^2 + ‖convexProj K hK_ne hK_cl hK_conv y - z‖^2 + 2 * inner ℝ (y - convexProj K hK_ne hK_cl hK_conv y) (convexProj K hK_ne hK_cl hK_conv y - z) := by
      rw [ show y - z = ( y - convexProj K hK_ne hK_cl hK_conv y ) + ( convexProj K hK_ne hK_cl hK_conv y - z ) by abel1, norm_add_sq_real ] ; ring;
    norm_num [ inner_sub_left, inner_sub_right ] at *;
    nlinarith [ norm_nonneg ( y - convexProj K hK_ne hK_cl hK_conv y ), norm_nonneg ( convexProj K hK_ne hK_cl hK_conv y - z ), norm_nonneg ( y - z ) ]

/-
First-order condition for convex functions involving the gradient.
-/
lemma convex_first_order_condition {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]
  (K : Set E) (hK_conv : Convex ℝ K)
  (f : E → ℝ) (hf_conv : ConvexOn ℝ K f)
  (x : E) (hx : x ∈ K) (y : E) (hy : y ∈ K)
  (hf_diff : DifferentiableAt ℝ f x) :
  f x + inner ℝ (gradient f x) (y - x) ≤ f y := by
    rw [ gradient ];
    -- By definition of the derivative, we know that
    have h_deriv : Filter.Tendsto (fun t : ℝ => (f (x + t • (y - x)) - f x) / t) (nhdsWithin 0 (Set.Ioi 0)) (nhds (fderiv ℝ f x (y - x))) := by
      have h_deriv : HasDerivAt (fun t : ℝ => f (x + t • (y - x))) ((fderiv ℝ f x) (y - x)) 0 := by
        have h_chain : HasDerivAt (fun t : ℝ => x + t • (y - x)) (y - x) 0 := by
          simpa using hasDerivAt_id ( 0 : ℝ ) |> HasDerivAt.smul_const <| y - x
        convert HasFDerivAt.comp_hasDerivAt _ _ h_chain using 1;
        simpa using hf_diff.hasFDerivAt;
      simpa [ div_eq_inv_mul ] using h_deriv.tendsto_slope_zero_right;
    -- By the properties of the derivative and the convexity of $f$, we have
    have h_ineq : ∀ t ∈ Set.Ioo 0 1, (f (x + t • (y - x)) - f x) / t ≤ f y - f x := by
      intro t ht
      have h_ineq : f (x + t • (y - x)) ≤ (1 - t) * f x + t * f y := by
        convert hf_conv.2 hx hy ( by linarith [ ht.1, ht.2 ] : 0 ≤ 1 - t ) ( by linarith [ ht.1, ht.2 ] : 0 ≤ t ) ( by linarith [ ht.1, ht.2 ] ) using 1 ; simp +decide [ sub_smul ];
        exact congr_arg f ( by rw [ sub_add_eq_add_sub ] ; rw [ smul_sub ] ; abel1 );
      rw [ div_le_iff₀ ] <;> linarith [ ht.1, ht.2 ];
    -- Taking the limit of the inequality as $t$ approaches $0$ from the right, we get
    have h_lim_ineq : (fderiv ℝ f x (y - x)) ≤ f y - f x := by
      exact le_of_tendsto h_deriv ( Filter.eventually_of_mem ( Ioo_mem_nhdsGT_of_mem ⟨ by norm_num, by norm_num ⟩ ) h_ineq );
    simp_all +decide [ inner_smul_left, inner_smul_right, fderiv_deriv ];
    linarith

/-
Descent lemma for Projected Gradient Descent step.
-/
lemma descent_step {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]
  (K : Set E) (hK_ne : K.Nonempty) (hK_cl : IsClosed K) (hK_conv : Convex ℝ K)
  (f : E → ℝ) (hf_conv : ConvexOn ℝ K f)
  (x : E) (hx : x ∈ K) (x_star : E) (hx_star : x_star ∈ K)
  (η : ℝ) (hη : 0 < η)
  (L : ℝ) (h_grad : ‖gradient f x‖ ≤ L)
  (hf_diff : DifferentiableAt ℝ f x) :
  ‖pgd_step K hK_ne hK_cl hK_conv f η x - x_star‖^2 ≤ ‖x - x_star‖^2 - 2 * η * (f x - f x_star) + η^2 * L^2 := by
    -- Let $z = \Pi_K(x - \eta \nabla f(x))$.
    set z := pgd_step K hK_ne hK_cl hK_conv f η x;
    -- By the properties of the projection, we have $\|z - x^*\|^2 \leq \|(x - \eta \nabla f(x)) - x^*\|^2$.
    have h_proj : ‖z - x_star‖ ^ 2 ≤ ‖(x - η • gradient f x) - x_star‖ ^ 2 := by
      have := norm_convexProj_sub_le K hK_ne hK_cl hK_conv x_star hx_star;
      exact pow_le_pow_left₀ ( norm_nonneg _ ) ( this _ ) _;
    -- By the properties of the projection, we have $\|(x - \eta \nabla f(x)) - x^*\|^2 = \|x - x^*\|^2 - 2\eta \langle \nabla f(x), x - x^* \rangle + \eta^2 \|\nabla f(x)\|^2$.
    have h_expand : ‖(x - η • gradient f x) - x_star‖ ^ 2 = ‖x - x_star‖ ^ 2 - 2 * η * inner ℝ (gradient f x) (x - x_star) + η ^ 2 * ‖gradient f x‖ ^ 2 := by
      norm_num [ norm_sub_sq_real, inner_sub_left, inner_sub_right ] ; ring;
      simp +decide [ norm_smul, inner_smul_left, inner_smul_right ] ; ring;
      rw [ abs_of_pos hη ] ; rw [ real_inner_comm ] ; ring;
    -- By the properties of the projection, we have $\langle \nabla f(x), x - x^* \rangle \geq f(x) - f(x^*)$.
    have h_inner : inner ℝ (gradient f x) (x - x_star) ≥ f x - f x_star := by
      have := convex_first_order_condition K hK_conv f hf_conv x hx x_star hx_star hf_diff;
      rw [ inner_sub_right ] at this;
      rw [ inner_sub_right ] ; linarith;
    refine le_trans h_proj ?_;
    rw [ h_expand ];
    gcongr

/-
Bound on the sum of function value differences for PGD.
-/
lemma pgd_sum_bound {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]
  (K : Set E) (hK_ne : K.Nonempty) (hK_cl : IsClosed K) (hK_conv : Convex ℝ K)
  (f : E → ℝ) (hf_conv : ConvexOn ℝ K f)
  (x_star : E) (hx_star : x_star ∈ K)
  (η : ℝ) (hη : 0 < η)
  (L : ℝ) (hL : 0 < L)
  (x₀ : E) (hx₀ : x₀ ∈ K)
  (T : ℕ)
  (h_grad : ∀ t < T, ‖gradient f (pgd_sequence K hK_ne hK_cl hK_conv f η x₀ t)‖ ≤ L)
  (h_diff : ∀ t < T, DifferentiableAt ℝ f (pgd_sequence K hK_ne hK_cl hK_conv f η x₀ t)) :
  ∑ t ∈ Finset.range T, (f (pgd_sequence K hK_ne hK_cl hK_conv f η x₀ t) - f x_star) ≤
    ‖x₀ - x_star‖^2 / (2 * η) + (T : ℝ) * η * L^2 / 2 := by
      -- Apply the descent lemma for each $t < T$.
      have h_descend : ∀ t < T, ‖pgd_sequence K hK_ne hK_cl hK_conv f η x₀ (t + 1) - x_star‖^2 ≤ ‖pgd_sequence K hK_ne hK_cl hK_conv f η x₀ t - x_star‖^2 - 2 * η * (f (pgd_sequence K hK_ne hK_cl hK_conv f η x₀ t) - f x_star) + η^2 * L^2 := by
        intro t ht;
        convert descent_step K hK_ne hK_cl hK_conv f hf_conv _ _ x_star hx_star η hη L ( h_grad t ht ) ( h_diff t ht ) using 1;
        induction' t with t ih;
        · exact hx₀;
        · exact convexProj_mem K hK_ne hK_cl hK_conv _;
      -- By summing the inequalities from the descent lemma over all $t < T$, we obtain:
      have h_sum_descend : ‖pgd_sequence K hK_ne hK_cl hK_conv f η x₀ T - x_star‖^2 ≤ ‖x₀ - x_star‖^2 - 2 * η * ∑ t ∈ Finset.range T, (f (pgd_sequence K hK_ne hK_cl hK_conv f η x₀ t) - f x_star) + T * η^2 * L^2 := by
        induction' T with T ih;
        · norm_num +zetaDelta at *;
          rfl;
        · rw [ Finset.sum_range_succ ];
          exact le_trans ( h_descend T ( Nat.lt_succ_self T ) ) ( by push_cast; linarith [ ih ( fun t ht => h_grad t ( Nat.lt_succ_of_lt ht ) ) ( fun t ht => h_diff t ( Nat.lt_succ_of_lt ht ) ) ( fun t ht => h_descend t ( Nat.lt_succ_of_lt ht ) ) ] );
      rw [ div_add_div, le_div_iff₀ ] <;> nlinarith [ norm_nonneg ( pgd_sequence K hK_ne hK_cl hK_conv f η x₀ T - x_star ) ]

/-
Algebraic bound for PGD convergence proof.
-/
lemma algebraic_bound (R L ε : ℝ) (T : ℕ) (η : ℝ)
  (hR : 0 ≤ R) (hL : 0 < L) (hε : 0 < ε)
  (hT : T = ⌈(L^2 * R^2) / ε^2⌉₊)
  (hη : η = R / (L * Real.sqrt T))
  (hT_pos : 0 < T) :
  (1 / ((T : ℝ) + 1)) * (R^2 / (2 * η) + ((T : ℝ) + 1) * η * L^2 / 2) ≤ ε := by
    subst hη;
    -- We'll use that $T \geq L^2 R^2 / \epsilon^2$ to simplify our expression.
    have h_T_ge : (T : ℝ) ≥ L^2 * R^2 / ε^2 := by
      exact hT.symm ▸ Nat.le_ceil _;
    -- Simplify the expression inside the inequality.
    field_simp [mul_comm, mul_assoc, mul_left_comm] at *;
    rw [ Real.sq_sqrt ( Nat.cast_nonneg _ ) ];
    -- Squaring both sides to remove the square root.
    have h_sq : (R * L * (2 * T + 1))^2 ≤ (2 * Real.sqrt T * (T + 1) * ε)^2 := by
      ring_nf;
      rw [ Real.sq_sqrt ] <;> nlinarith [ show ( T : ℝ ) ≥ 1 by norm_cast ];
    nlinarith [ show 0 ≤ 2 * Real.sqrt T * ( T + 1 ) * ε by positivity ]

/-
Definition of the average of the PGD sequence using List and let bindings.
-/
noncomputable def pgd_average {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]
  (K : Set E) (hK_ne : K.Nonempty) (hK_cl : IsClosed K) (hK_conv : Convex ℝ K)
  (f : E → ℝ) (η : ℝ) (x₀ : E) (T : ℕ) : E :=
  let range := List.range (T + 1)
  let vals := range.map (pgd_sequence K hK_ne hK_cl hK_conv f η x₀)
  (1 / ((T : ℝ) + 1)) • vals.sum

/-
Definition of the average of the PGD sequence using List.
-/
noncomputable def pgd_avg_final {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]
  (K : Set E) (hK_ne : K.Nonempty) (hK_cl : IsClosed K) (hK_conv : Convex ℝ K)
  (f : E → ℝ) (η : ℝ) (x₀ : E) (T : ℕ) : E :=
  let l := List.range (T + 1)
  let s := l.map (fun t => pgd_sequence K hK_ne hK_cl hK_conv f η x₀ t)
  (1 / ((T : ℝ) + 1)) • s.sum

/-
Bound on the function value at the average iterate of PGD.
-/
lemma pgd_average_bound {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]
  (K : Set E) (hK_ne : K.Nonempty) (hK_cl : IsClosed K) (hK_conv : Convex ℝ K)
  (f : E → ℝ) (hf_conv : ConvexOn ℝ K f)
  (x_star : E) (hx_star : x_star ∈ K)
  (η : ℝ) (hη : 0 < η)
  (L : ℝ) (hL : 0 < L)
  (x₀ : E) (hx₀ : x₀ ∈ K)
  (T : ℕ)
  (h_grad : ∀ t < T + 1, ‖gradient f (pgd_sequence K hK_ne hK_cl hK_conv f η x₀ t)‖ ≤ L)
  (h_diff : ∀ t < T + 1, DifferentiableAt ℝ f (pgd_sequence K hK_ne hK_cl hK_conv f η x₀ t)) :
  f (pgd_average K hK_ne hK_cl hK_conv f η x₀ T) - f x_star ≤
    (1 / ((T : ℝ) + 1)) * (‖x₀ - x_star‖^2 / (2 * η) + ((T : ℝ) + 1) * η * L^2 / 2) := by
      -- By definition of `pgd_average`, we can rewrite the goal using the average of the PGD sequence.
      suffices h_avg_bound : (∑ t ∈ Finset.range (T + 1), f (pgd_sequence K hK_ne hK_cl hK_conv f η x₀ t)) - (T + 1) * f x_star ≤
          ‖x₀ - x_star‖^2 / (2 * η) + ((T + 1) * η * L^2) / 2 by
            -- By definition of `pgd_average`, we can rewrite the goal using the average of the PGD sequence and apply Jensen's inequality.
            have h_jensen : f (∑ t ∈ Finset.range (T + 1), (1 / (T + 1 : ℝ)) • (pgd_sequence K hK_ne hK_cl hK_conv f η x₀ t)) ≤ ∑ t ∈ Finset.range (T + 1), (1 / (T + 1 : ℝ)) * f (pgd_sequence K hK_ne hK_cl hK_conv f η x₀ t) := by
              apply_rules [ hf_conv.map_sum_le ];
              · exact fun _ _ => by positivity;
              · simp +decide [ Nat.cast_add_one_ne_zero ];
              · intro t ht;
                exact Nat.recOn t hx₀ fun n hn => convexProj_mem _ hK_ne hK_cl hK_conv _;
            simp_all +decide [ ← Finset.mul_sum _ _ _, ← Finset.sum_mul ];
            convert h_jensen.trans ( mul_le_mul_of_nonneg_left h_avg_bound ( by positivity ) ) using 1 ; ring!;
            · unfold pgd_average; simp +decide [ add_comm, ← Finset.sum_smul ] ;
              rw [ ← Finset.smul_sum ];
              congr;
            · field_simp;
      -- Apply the descent lemma to each term in the sum.
      have h_sum_bound : ∀ t < T + 1, f (pgd_sequence K hK_ne hK_cl hK_conv f η x₀ t) - f x_star ≤ (‖pgd_sequence K hK_ne hK_cl hK_conv f η x₀ t - x_star‖^2 - ‖pgd_sequence K hK_ne hK_cl hK_conv f η x₀ (t + 1) - x_star‖^2) / (2 * η) + (η * L^2) / 2 := by
        intro t ht
        have h_step : ‖pgd_sequence K hK_ne hK_cl hK_conv f η x₀ (t + 1) - x_star‖^2 ≤ ‖pgd_sequence K hK_ne hK_cl hK_conv f η x₀ t - x_star‖^2 - 2 * η * (f (pgd_sequence K hK_ne hK_cl hK_conv f η x₀ t) - f x_star) + η^2 * L^2 := by
          apply descent_step K hK_ne hK_cl hK_conv f hf_conv (pgd_sequence K hK_ne hK_cl hK_conv f η x₀ t) (by
          exact Nat.recOn t hx₀ fun t ih => convexProj_mem K hK_ne hK_cl hK_conv _) x_star hx_star η hη L (h_grad t ht) (h_diff t ht);
        rw [ div_add_div, le_div_iff₀ ] <;> nlinarith;
      have h_sum_bound : ∑ t ∈ Finset.range (T + 1), (f (pgd_sequence K hK_ne hK_cl hK_conv f η x₀ t) - f x_star) ≤ ∑ t ∈ Finset.range (T + 1), ((‖pgd_sequence K hK_ne hK_cl hK_conv f η x₀ t - x_star‖^2 - ‖pgd_sequence K hK_ne hK_cl hK_conv f η x₀ (t + 1) - x_star‖^2) / (2 * η) + (η * L^2) / 2) := by
        exact Finset.sum_le_sum fun t ht => h_sum_bound t <| Finset.mem_range.mp ht;
      -- Notice that the sum of the differences of norms is telescoping.
      have h_telescope : ∑ t ∈ Finset.range (T + 1), (‖pgd_sequence K hK_ne hK_cl hK_conv f η x₀ t - x_star‖^2 - ‖pgd_sequence K hK_ne hK_cl hK_conv f η x₀ (t + 1) - x_star‖^2) = ‖x₀ - x_star‖^2 - ‖pgd_sequence K hK_ne hK_cl hK_conv f η x₀ (T + 1) - x_star‖^2 := by
        convert Finset.sum_range_sub' _ _ using 3;
      simp_all +decide [ Finset.sum_add_distrib, ← Finset.sum_div _ _ _ ];
      exact h_sum_bound.trans ( by rw [ sub_div, mul_div_assoc ] ; ring_nf; norm_num; positivity )

/-
Algebraic bound for PGD convergence proof (version 2).
-/
lemma algebraic_bound_v2 (R L ε : ℝ) (T : ℕ) (η : ℝ)
  (hR : 0 ≤ R) (hL : 0 < L) (hε : 0 < ε)
  (hT : T = ⌈(L^2 * R^2) / ε^2⌉₊)
  (hη : η = R / (L * Real.sqrt (T + 1))) :
  (1 / ((T : ℝ) + 1)) * (R^2 / (2 * η) + ((T : ℝ) + 1) * η * L^2 / 2) ≤ ε := by
    subst hη;
    -- Simplify the expression inside the parentheses.
    field_simp;
    -- By simplifying, we can see that the inequality holds.
    have h_simp : R * L ≤ Real.sqrt (T + 1) * ε := by
      have := Nat.le_ceil ( L ^ 2 * R ^ 2 / ε ^ 2 );
      rw [ div_le_iff₀ ( by positivity ) ] at this;
      nlinarith [ show 0 ≤ Real.sqrt ( T + 1 ) * ε by positivity, Real.mul_self_sqrt ( show 0 ≤ ( T : ℝ ) + 1 by positivity ), show ( ⌈L ^ 2 * R ^ 2 / ε ^ 2⌉₊ : ℝ ) ≤ T by rw [ hT ] ];
    rw [ Real.sq_sqrt ] <;> nlinarith [ Real.sqrt_nonneg ( T + 1 : ℝ ), Real.mul_self_sqrt ( by positivity : 0 ≤ ( T : ℝ ) + 1 ) ]

/-
Bound on the function value at the average iterate of PGD (expanded definition).
-/
lemma pgd_avg_bound_expanded {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]
  (K : Set E) (hK_ne : K.Nonempty) (hK_cl : IsClosed K) (hK_conv : Convex ℝ K)
  (f : E → ℝ) (hf_conv : ConvexOn ℝ K f)
  (x_star : E) (hx_star : x_star ∈ K)
  (η : ℝ) (hη : 0 < η)
  (L : ℝ) (hL : 0 < L)
  (x₀ : E) (hx₀ : x₀ ∈ K)
  (T : ℕ)
  (h_grad : ∀ t < T + 1, ‖gradient f (pgd_sequence K hK_ne hK_cl hK_conv f η x₀ t)‖ ≤ L)
  (h_diff : ∀ t < T + 1, DifferentiableAt ℝ f (pgd_sequence K hK_ne hK_cl hK_conv f η x₀ t)) :
  f ((1 / ((T : ℝ) + 1)) • ∑ t ∈ Finset.range (T + 1), pgd_sequence K hK_ne hK_cl hK_conv f η x₀ t) - f x_star ≤
    (1 / ((T : ℝ) + 1)) * (‖x₀ - x_star‖^2 / (2 * η) + ((T : ℝ) + 1) * η * L^2 / 2) := by
      exact?

/-
Jensen's inequality for the PGD average.
-/
lemma pgd_jensen {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]
  (K : Set E) (hK_ne : K.Nonempty) (hK_cl : IsClosed K) (hK_conv : Convex ℝ K)
  (f : E → ℝ) (hf_conv : ConvexOn ℝ K f)
  (η : ℝ) (x₀ : E) (hx₀ : x₀ ∈ K) (T : ℕ) :
  f (pgd_average K hK_ne hK_cl hK_conv f η x₀ T) ≤
  (1 / ((T : ℝ) + 1)) * ∑ t ∈ Finset.range (T + 1), f (pgd_sequence K hK_ne hK_cl hK_conv f η x₀ t) := by
    have h_jensen : f (∑ t ∈ Finset.range (T + 1), (1 / ((T : ℝ) + 1)) • pgd_sequence K hK_ne hK_cl hK_conv f η x₀ t) ≤ ∑ t ∈ Finset.range (T + 1), (1 / ((T : ℝ) + 1)) • f (pgd_sequence K hK_ne hK_cl hK_conv f η x₀ t) := by
      apply_rules [ hf_conv.map_sum_le ];
      · exact fun _ _ => by positivity;
      · simp +decide [ Nat.cast_add_one_ne_zero ];
      · intro i hi;
        induction' i with i ih;
        · exact hx₀;
        · exact convexProj_mem K hK_ne hK_cl hK_conv _;
    simp_all +decide [ Finset.mul_sum _ _ _, smul_eq_mul ];
    convert h_jensen using 2;
    exact ( by rw [ show pgd_average K hK_ne hK_cl hK_conv f η x₀ T = ( 1 / ( T + 1 : ℝ ) ) • ( ∑ t ∈ Finset.range ( T + 1 ), pgd_sequence K hK_ne hK_cl hK_conv f η x₀ t ) by rfl ] ; simp +decide [ smul_smul, Finset.smul_sum ] )

/-
If eta is 0, the PGD sequence is constant.
-/
lemma pgd_sequence_constant_of_eta_zero {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]
  (K : Set E) (hK_ne : K.Nonempty) (hK_cl : IsClosed K) (hK_conv : Convex ℝ K)
  (f : E → ℝ) (x₀ : E) (hx₀ : x₀ ∈ K) (n : ℕ) :
  pgd_sequence K hK_ne hK_cl hK_conv f 0 x₀ n = x₀ := by
    -- Since η = 0, the step size is zero, so the projection of x₀ - 0 • gradient f x₀ is just x₀.
    have h_proj : convexProj K hK_ne hK_cl hK_conv x₀ = x₀ := by
      have := convexProj_variational_inequality K hK_ne hK_cl hK_conv x₀ x₀ hx₀;
      rw [ real_inner_self_nonpos ] at this;
      exact Eq.symm ( sub_eq_zero.mp this );
    induction' n with n ih;
    · rfl;
    · rw [pgd_sequence];
      rw [pgd_step, ih]
      simp [h_proj]
